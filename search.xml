<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python爬虫系统学习笔记一]]></title>
    <url>%2F2017%2F09%2F16%2Fpython%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[在中国大学MOOC那发现了python的爬虫专题系列，简直就像捡到了宝一样，先来总结下第一周课的内容 Requests库入门​ 在前两次的爬虫体验里面已经用到了这个牛X的库，现在开始对它进行系统点的学习（emmm，个人不是很喜欢看文档，喜欢视频），嵩天老师讲的超级好，之前python入门也看过一点他的视频，感觉超级赞。废话貌似说的有点多。。。。 Requests库安装及七个基本方法​ python提供了安装包管理工具pip，有点类似于Sublime text的Package Control，pip安装的时候注意版本的兼容，建议选择源码编译安装，解压的个人觉得容易出问题，具体请自行百度。有了pip以后就简单了，打开cmd直接输入键入命令pip install requests(此处针对的是windows环境，mac什么的应该更加方便就不说了)。然后检查一下是否已经成功安装。 1234import requestsr = requests.get("http://www.baidu.com")print(r.status_code)print(r.text) 这里我用的是Sublime Text，其实用python自带的idle也挺方便的，但我个人不习惯呃。 如果打印出状态码为200而且输出的百度的文本内容，说明已经requests库已经到位，否则请回去检查一下安装是否出了问题。 其中requests.request()是基础，构造请求。然后其他6个分别是： get：获取HTML网页的主要方法 head: 获取HTML头部信息。 这是最为重要的两个，还有post, put, patch, delete。这6个方法均对应着HTTP协议的操作(可以说requests库就是为HTTP而生的)。 requests.request()​ 标准形式是：requests.request(method, url, **kwargs)，method对应上面的post, put等方法，而**keargs是控制参数。比如这样写： 123kv = &#123;'key1': 'value1','key2': 'value2'&#125;r = requests.request('GET', 'url', params = kv)print(r.url) 会打印出url?key1=value1&amp;key2=value2。最重要的是伪造header，这样写： 12hd = &#123;'user-agent':'Mozilla/5.0'&#125;r = requests.request('GET', 'url', heaers = hd) 其他还有json，data, cookies, auth, timeout等。 requests.get()​ 最常见的写法：r = requests.get(url),r表示的是服务器返回的一个Response对象，url即目标网页，完整的写法是这样的：r = requests.get(url, params = None, **kwargs)，后面两个为选填参数，其中params表示字典或者字节流格式，**keargs表示12个控制的参数。 Response对象的属性​ 有意思的是，response(下面简称r)对象不仅包括服务器返回的信息，也包括requests请求的信息。r的属性主要有这么5个： r.status_code: 200表示成功，404表示失败，更多状态码自己查查看。 r.text: HTTP响应内容的字符串形式。 r.encoding: 编码方式 r.apparent_encoding: 备选编码方式 r.content: HTTP响应内容的二进制形式 其中encoding是根据header里面的charset字段来判断的，如果没有，会默认为ISO-8859-1，此编码下中文会乱码，备选编码方式一般是utf-8。 Requests库的异常​ 嵩老师说了一句：网络连接有风险，异常处理很重要。错误有很多种，DNS查询失败，HTTP 错误，URL错误，超过最大重定向次数导致重定向异常，超时错误等等。只要状态不是200就可以认为是HTTP错误了。 通用代码框架1234567891011121314import requestsdef getHTMLText(url): try: r = requests.get(url) r.raise_for_status()#不是200就会引发异常 r.encoding = r.apparent_encoding return r.text except: return "异常"if __name__ == "__main__": url = "" print(getHTMLText(url)) ​ 不得不说，这代码比较严谨，不像我前面两次的那样，哈哈哈，从开始就养成好习惯。 盗亦有道——robots协议##爬虫尺度分类与可能带来的问题 ​ 网络爬虫是有尺度的，大概可以分为这么三种： 规模小，数据量小，速度不敏感，用requests库即能完成，用来玩转网页的。 中等规模，爬取速度敏感，用scrapy库，爬取系列网站。 大规模，用搜索引擎，定制开发。爬取全网 网络爬虫会带来的三个问题： 性能骚扰：水平太烂的话会给服务器资源造成巨大的压力 法律风险：如果爬来的数据用于商业用途可能要面对法律风险 隐私泄露；爬到正常情况下无权限获取的数据可能会造成个人隐私泄露 网络爬虫的限制来源审查：判断User-Agent，只允许友好爬虫和浏览器访问 发布公告：robots协议，告知爬虫，要求遵守。 robots协议全名叫：Robots Exclusion Standard，网络爬虫排除标准。以协议约定的形式告知爬虫爬取权限，一般在网站的根目录下。来看一下京东的robots协议： 12345678910111213&gt;User-agent: * &gt;Disallow: /?* &gt;Disallow: /pop/*.html &gt;Disallow: /pinpai/*.html?* &gt;User-agent: EtaoSpider &gt;Disallow: / &gt;User-agent: HuihuiSpider &gt;Disallow: / &gt;User-agent: GwdangSpider &gt;Disallow: / &gt;User-agent: WochachaSpider &gt;Disallow: /&gt; *代表所有，/代表根目录 原则上是应该要遵守robots协议的，但如果访问量很小，类似人的上网行为，可以不考虑遵守robots协议。但是！！！如果是商业用途或者爬取全网，必须遵守robots协议，不然出了问题是真的要背法律责任的。 举个栗子保存一张图片： 12345678910111213141516171819import requestsimport osurl = "https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1832529435,1519198910&amp;fm=11&amp;gp=0.jpg"root = "D://pics//"path = root + url.split('/')[-1]try: if not os.path.exists(root): os.mkdir(root) if not os.path.exists(path): r = requests.get(url) with open(path, 'wb') as f: f.write(r.content) f.close() print("文件保存成功") else: print("文件已存在")except: print("爬取失败") root是目录，先检测目录是否存在，如果没有，建一个，然后路径同理，接着打开一个个文件，然后写入r.content(因为content属性是二进制形式的) 嗯？保存一张图片也许没有右键另存为来的快，但成百上千张图片可就无能为力了，特别是还要对图片进行筛选的情况下。(我存的图片男生可以参考，女生可以换一张，哈哈哈哈哈哈哈) Beautiful Soup库入门​ 懒得再新建文章了，直接继续第二章的内容。这次是学习Beautiful Soup库，(形象地称之为煲汤。。。。是的，没错，老师也用了这个词。。。。)自行安装 测试是否安装成功： 1234567import requestsfrom bs4 import BeautifulSoupr = requests.get("http://python123.io/ws/demo.html")demo = r.textsoup = BeautifulSoup(demo, 'html.parser')print(soup.prettify()) 如果打印出来的内容和HTML文档排版一样就OK。 ##Beautiful Soup库的基本元素 ​ 首先，Beautiful Soup库是解析、遍历、维护“标签树”的功能库，这里会对应HTML的框架(Requests库对应HTTP操作)。 ​ Beautiful Soup库的引用一般是BeautifulSoup类，有意思的是，真正用的时候只要写bs4就好，from bs4 import BeautifulSoup,就是这样奇葩。而上面代码中soup = BeautifulSoup(demo, &#39;html.parser&#39;),html.parser则指的是html解析器。用来解析demo的HTML结构。 ​ Beautiful Soup库有如下5个基本元素： Tag: 最基本的信息单元 Name: 标签的名字，格式：.name Attributes: 标签的属性。格式：.attrs NavigableString: 标签内非属性字符串。格式：.string Comment: 标签内部的注释信息，一种特殊的comment类型 Tag 标签​ 任何HTML语法中的标签都可以通过soup.tag访问，比如，soup.title，soup.a等等当存在多个相同的tag时，返回第一个 ​ 每个标签都有自己的名字，可以这样来访问：soup.a.name，还可以查看其上一级标签名字soup.a.parent.name，父亲的父亲同理，儿子的话用children就行. ​ 标签属性是字典形式，可以这样来访问：tag = soup.a,tag.attrs，打印出一个字典后，还可以查看具体的某一个属性，比如tag.attrs[&#39;class&#39;]。 ​ NavigableString 和 Comment不再细说，原理和这个一样。 基于bs4库的HTML内容遍历方法​ 贴一下HTML的基本结构 123456789101112131415161718192021222324&lt;html&gt; &lt;head&gt; &lt;title&gt; This is a python demo page &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="title"&gt; &lt;b&gt; The demo python introduces several python courses. &lt;/b&gt; &lt;/p&gt; &lt;p class="course"&gt; Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses: &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt; Basic Python &lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt; Advanced Python &lt;/a&gt; &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 标签树的三个属性可以做到下行遍历： .contents：将tag的所有子节点都存入列表 .children：子节点的迭代类型，用循环遍历子节点 .descendants：子孙节点的迭代烈性，包括所有子孙节点，用循环遍历 遍历子节点和子孙节点： 12345for child in soup.body.children: print(child) for child in soup.body.descendants: print(child) ​ 上行遍历： .parent：节点的父节点 .parents：节点父辈的迭代类型，循环遍历 上行遍历示例： 12345for parent in soup.a.parents: if parent is None: print(parent) else: print(parent.name) 考虑到会遍历到根节点(即soup本身)，所以做了一个判断。 平行遍历： .next_sibling：按照顺序遍历下一个平行节点 .previous_sibling：按照顺序遍历上一个平行节点 .next_siblings：迭代，循环遍历后面所有平行节点 .previous_siblings：迭代，循环遍历前面所有平行节点 注意一个问题：在同一个父节点下才算是平行节点。 比如，这是遍历后续节点： 12for sibling in soup.a.next_sibling: print(sibling) 友好输出：Prettify()​ 很简单，直接print(soup.prettify())，而且支持utf-8编码. 信息标记与提取##信息标记 ​ 上表就好： XML：最早的通用信息标记语言，可扩展性强但是繁琐 JSON：信息有类型，键值对显示，超级适合js的处理，比XML简洁 YAML：信息无类型，文本信息比例高，可读性很好。 作用： XML：Internet上的信息交互与传递 JSON：移动应用端与节点的信息通信 YAML：系统的配置文件 信息提取​ 最好的方法当然是形式解析与关键字内容搜索融合。 ​ 比如，要提取所有的URL链接，只需要这样： 12for link in soup.find_all('a'): print(link.get('href')) ​ find_all(name, attrs, recursive, string, **kwargs)，参数分别是标签名字，标签属性，是否对子孙全部检索(默认为True)，检索区域，及控制参数，既然是匹配，肯定是正则大法好咯。TIPS：(…) 等价于 .find_all()，soup(…) 等价于 soup.find_all() 再举个栗子​ 爬取中国大学排名，很简单的栗子，直接上代码好了： 1234567891011121314151617181920212223242526272829303132333435import requestsfrom bs4 import BeautifulSoupimport bs4def getHTMLText(url): try: r = requests.get(url, timeout = 30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return ""def fillUnivList(ulist, html): soup = BeautifulSoup(html, "html.parser") for tr in soup.find('tbody').children: if isinstance(tr, bs4.element.Tag): tds = tr('td') ulist.append([tds[0].string, tds[1].string, tds[3].string])def printUnivList(ulist, num): tplt = "&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;" print(tplt.format("排名", "学校名称", "总分",chr(12288))) for i in range(num): u = ulist[i] print(tplt.format(u[0], u[1], u[2], chr(12288)))def main(): unifo = [] url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html' html = getHTMLText(url) fillUnivList(unifo, html) printUnivList(unifo, 30)#30所大学main() ​ tplt和chr(12288)是为了使输出稍微美观一点而做的一点小优化。中间要判断是否是bs4.element.tag类型，所以引用了bs4库。(吐槽一下python的format库威力是真的强，还有，南航居然排到了29诶。。。)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取网易云热评]]></title>
    <url>%2F2017%2F09%2F14%2F%E7%88%AC%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E7%83%AD%E8%AF%84%2F</url>
    <content type="text"><![CDATA[爬取网易云热评，用python上手真心简单，怪不得有人总说“人生苦短，我用python“。 ​ 爬网页数据的话一般是解析html的结构，这种适合抓取html里面多种元素的情况，而我只是想看个热评而已，可以另辟蹊径：直接搞到评论的API，然后获取json返回，最后解析就行了。((￣▽￣)~所以说很简单咦) 找到API​ 打开网易云网页版，输入自己想抓取热评的歌曲，然后用开发者工具，输入comments就能找到评论API的url了，点response就能看到json格式的评论了。 那个数字代表的应该就是这首歌的id了，后面有一个叫csrf_tocken的参数，显示为空，那就不管它了。请求方式依然是post，然后表单那里还有两个加密过的参数，分别是params和encSecKey，一开始以为是每首歌都会对应这么一个参数，后来发现并不是，刷新了页面后却发现这个参数变了，可能是对应评论页码加密的？ 获取json​ 再一次慨叹一下python库的强大，不信？那你看代码长度咯。 1234567import requestsimport jsonurl = 'http://music.163.com/weapi/v1/resource/comments/R_SO_4_63650?csrf_token='param = &#123;'params':'', 'encSecKey':''&#125;r = requests.post(url, param)data = r.text #拿到json 这样就拿到评论的json格式了(PS：那两个参数实在是太长了，贴出来看的我不舒服，写的时候自行粘贴上去就好)。 解析json拿到了json后自然要解析一波，python的json库可以很轻易地帮我们解决这个问题。首先从开发者工具那直接复制json，拿到这里在线json校验格式化工具,看一下json的具体形式(直接那么长一串简直反人类没法看)。 舒服多了23333。主要信息有nickname，content，likedCount把这三个拎出来就差不多了。(id, 内容, 点赞量)json库不会用的话去查一下就好，上手很容易的，然后直接解析后打印就好了。(就一页热评，懒得写入文件了。)稍微完整的代码： 1234567891011121314151617import requestsimport jsonurl = 'http://music.163.com/weapi/v1/resource/comments/R_SO_4_63650?csrf_token='param = &#123;'params':'', 'encSecKey':''&#125;r = requests.post(url, param)data = r.text #拿到jsonjsOb = json.loads(data)hotComments = jsOb['hotComments']for i in range(len(hotComments)): user_nickname = hotComments[i]['user']['nickname'] likedCount = hotComments[i]['likedCount'] content = hotComments[i]['content'] print('评论',str(i+1),' 用户名:',user_nickname,'喜 欢：',str(likedCount)) print('-----------------------------') print(content) 这样就OK咯。贴一下我抓的《独家记忆》第一页热评。 emmmmm，后期再更新一下抓全部评论和整个歌单的热评好了。不过那样数据会比较大，时间相应也挺长的，不太敢多开线程加快速度，怕触发反爬虫机制ip被封了就GG了。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫初体验]]></title>
    <url>%2F2017%2F09%2F11%2Fpython%E7%88%AC%E8%99%AB%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[python基础部分看了一遍后，感觉需要找点乐子，于是想到去学一点爬虫批量抓图片，嘿嘿嘿 先介绍一下python的两大利器(库)：requests和re(emmmm，可别以为后面那个是前者的缩写，其实半毛钱关系都没有)。requests库有多厉害呢？简单地说就是python 的http库，可以帮开发人员省很多代码，具体有多厉害呢？戳这里 。而re库则是regexp的缩写，python提供了相当强大的正则表达式引擎，很大程度上简化了python代码，稍微详细一点的介绍在这里 。好了以下是正文 获取网页12345import requestsheader = &#123;'User-Agent':'xxx'&#125;url='https://www.xxxxx'r = requests.get(url,headers = header)txt = r.text 先引入 requests模块，然后header是用来伪造浏览器UA的，用Chrom的话直接去开发者工具那找一下自己的Request Headers就好，url则是要抓取的网页地址，传入这两个参数给.get函数后，就能拿到初步结果 解析图片嗯，别忘了自己是来抠图的，上面那个函数写完后，一个print(text)出来的却是一大坨HTML，这当然不是我要的，去网上找了一下发现Python还有个叫Beautiful Soup的东西(→_→我第一反应是煲汤)，这个库是用来解析HTML结构的，看了几眼觉得有点复杂，这时候想起了正则大法，图片链接不都是有明显特征的嘛，于是考虑正则。匹配以https://开头以.jpg类结尾的，并作最短匹配，然后全部累塞进数组。 12345678910jpg = re.compile(r'https://[^\s]*?\.jpg')jpeg = re.compile(r'https://[^\s]*?\.jpeg')gif = re.compile(r'https://[^\s]*?\.gif')png = re.compile(r'https://[^\s]*?\.png')imgs=[]imgs+=jpg.findall(txt)imgs+=jpeg.findall(txt)imgs+=gif.findall(txt)imgs+=png.findall(txt) 下载图片request用来存储get的url，判断状态码是否正常，如果正常就写入文件。 12345678910def download(url): request = requests.get(url) if req.status_code == requests.codes.ok: name = url.split('/')[-1] f = open("./"+name,'wb') f.write(req.content) f.close() return True else: return False 写好下载函数后就可以写循环来挨个下载了 12345678errors = []for img_url in imgs: if download(img_url): print("download :"+img_url) else: errors.append(img_url)print("error urls:")print(errors) 还可以设置一个urls数组，同时下载多个网页的图，download函数也可以加个文件夹名，这样就可以了。 最后我干了什么呢？上个图，自己体会 以上代码需要优化，直接用的话还有些不完善，需要根据实际的网站来调整。 多说一句​ 想起一位厉害小姐姐的训示：少年的你的梦想应该是拯救世界，怎么能每天爬一些日韩女人啊爆照贴妹子什么的。 ​ (逃：]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10改掉C盘下的中文用户名]]></title>
    <url>%2F2017%2F09%2F11%2Fwin10%E6%94%B9%E6%8E%89C%E7%9B%98%E4%B8%8B%E7%9A%84%E4%B8%AD%E6%96%87%E7%94%A8%E6%88%B7%E5%90%8D%2F</url>
    <content type="text"><![CDATA[昨天早上一起来，正愉快地折腾着，突然发现有些报错无论如何都解决不了，有的虽然影响不大但看起来很烦，搜索一波后发现其实是我c盘下中文路径搞的鬼， 嗯，话不多说就是干。 搜了一会儿后发现这个世界上的误解还真多，居然有这么多人以为是改本地的管理员账号，毫不客气地给了一个”踩”。终于在知乎下面找到了一点眉目。（还是比百度强） 懒得传图，直接复制一下第一个答案： 假设原用户名为 小明，需改为 xiaoming。 1.先新建一个管理员账户，然后注销当前用户，以新建的管理员账户登录； 2.重命名 c:\Users\小明 为 c:\Users\xiaoming； 3.打开注册表编辑器（win+R 输入 regedit），定位到 HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ProfileList 的某一子项（S-1-5-21… 开头的），将“数据名称”为 ProfileImagePath 的“数值数据”内容 C:\Users\小明 改为 C:\Users\xiaoming ； 4.改后以原账户登录并删除新建账户。 咦，下面有80+评论，去瞄了一眼，虽然成功的人也有，但失败的也不少，我还是没有直接按照这个方法改，但大概思路已经清楚了，即临时存一下当前中文名下的东西，拿到改名权限后改掉中文名，再换回来。 在下面的回答中找到了一位小姐姐的博客园链接，给链接的人称此方法很成功，嗯，大概说一下这个方法的步骤： 改注册表用win+r打开命令窗口，输入regedit，把系统注册表调出来，然后去这个地方 HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ProfileList 依次点开那些以S-1-5-开头的项，找到ProfileImagePath，把C：\Users\中文名 改成C：\Users\English name 记得把所有的Path都改掉，确认后关掉注册表。 改文件名打开cmd后发现依旧是中文名，这个当然，因为只是改完了注册表，文件权限还没转移，于是重启电脑。 启动后发现壁纸什么的都不一样了，这是因为当前Users 已经变成了TEMP，即临时系统账户。这时候系统会提醒你无法连接到你的账户，嗯，因为你的注册表已经改了，点击隐藏， 别点注销，然后按ctrl + e 打开我的电脑，找到C：\Users\中文名，修改文件名，点击给予权限。然后重启电脑就大功告成啦 可能有的问题更改之前就已经安装的软件可能依旧会显示中文路径，如果不报错的话就没关系，（强迫症选手请重装软件）。其他的问题好像没了，那个小姐姐还给了测试。 这是小姐姐的文章链接]]></content>
      <categories>
        <category>环境</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大和子序列]]></title>
    <url>%2F2017%2F09%2F10%2F%E6%9C%80%E5%A4%A7%E5%92%8C%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[今天来看一个简单的问题，求最大的和子序列/求最大和子数组 题目是这样的：已知序列：-2, 11, -4, 13, -5, 2, -5, -3, 12, -9，求此序列的最大子序列和 ​ 其实题目很简单，但智障的我一开始弄错了，直接把所有负数提出去然后把剩下的相加，这也太简单了点吧。。。。后来想想，貌似不太对，于是，重来。一共用了三种方法。(名字都是我瞎写的) 方法一：暴力法​ 没错，就是直接把这个数组的所有子序列的和都算一遍，跟初始最大值比较，代码如下： 12345678910111213141516171819int main()&#123; int a[] = &#123;-2,11,-4,13,-5,2,-5,-3,12,-9&#125;; int maxSum = a[0],n = sizeof(a)/sizeof(a[0]); for(int i = 0;i &lt; n;i++)//子数组长度 &#123; for(int j = 0;j &lt; n;j++)//子数组开始的位置，数组下标 &#123; int sum = 0;//记录当前子数组和 for(int k = j;k &lt; n&amp;&amp;k &lt; j + i;k++)//求和 &#123; sum += a[k]; &#125; if(sum &gt; maxSum) maxSum = sum; &#125; &#125; cout &lt;&lt; "子序列的最大和是："&lt;&lt; maxSum &lt;&lt; endl; return 0;&#125; 对，就是这么暴力，效率很低，时间复杂度：O(n³) 方法二：递进求和​ 不断求出以a[i]开头的子序列的和，并在求的过程中记录好最大的子序列的和，函数代码如下： 1234567891011121314int maxSubArraySum(int *arr,int n)&#123; int i,j,maxSum = 0,sum; for(i = 0;i &lt; n;i++)//子数组开始位置 &#123; sum = 0; for(j = i;j &lt; n;j++) &#123; sum += arr[j]; if(sum &gt; maxSum) maxSum = sum;//求和并比较 &#125; &#125; return maxSum; &#125; 相对方法一来说，方法二减少了一次遍历，时间复杂度为：O(n²) 方法三：判断求和​ 仔细想一下，一个数，加上一个负数会变小，加上零不变，加上正数才会变大，对，就是这么简单的道理，就可以用来优化这个题的算法了。从a[0]开始累加，如果大于初始值，就替换，如果和小于零，直接舍弃，然后是a[1]，函数代码如下： 1234567891011int maxSubArraySum_2(int *arr,int n)&#123; int i,maxSum = arr[0],sum = 0; for(i = 0;i &lt; n;i++)//子数组开始位置 &#123; sum += arr[i]; if(sum &gt; maxSum) maxSum = sum;//记录最大累加和 if(sum &lt; 0) sum = 0;//累加和小于零的不要 &#125; return maxSum;&#125; 这样的话，只要对数组遍历一次就能解决了，时间复杂度降为O(n)，最简单道理往往有意想不到的效果，哈哈哈哈。 另外，加个tips: n = sizeof(a)/sizeof(a[0])，Strlen()函数不适用于整数数组]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于排序]]></title>
    <url>%2F2017%2F09%2F10%2F%E5%85%B3%E4%BA%8E%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[我目前已经学会的排序以及理解（会不断更新） 先贴个交换函数： 123456void Swap(int A[],int i,int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125; 冒泡排序这是进大学后c语言课本上介绍的第一个排序，也是最简单，最容易理解的一个排序，顾名思义，就像冒泡一样，一次一次把最值往最后面放，完成排序。函数代码如下： 12345678910111213void BubbleSort(int A[],int n)&#123; for(int j = 0;j &lt; n - 1;j++) &#123; for(int i = 0;i &lt; n - 1 - j;i++) &#123; if(A[i] &gt; A[i + 1]) //从小到大排序 &#123; Swap(A,i,i + 1); &#125; &#125; &#125;&#125; 冒泡排序是稳定的，因为即使有相同的数也不会打乱原来是次序，平均时间复杂度：O(n²)。 选择排序​ 相比于相邻交换的冒牌排序，选择排序是通过从未排序的数据元素中选出最值放在该序列的起始位置，直到所有元素排完，同样需要循环两次，无法优化时间。代码如下： 123456789101112131415161718void SelectionSort(int A[],int n)&#123; for(int i = 0;i &lt; n - 1;i++) &#123; int min = i; for(int j = i + 1;j &lt; n;j++) &#123; if(A[j] &lt; A[min]) &#123; min = j; &#125; &#125; if(min != i) &#123; Swap(A,min,i); &#125; &#125;&#125; 选择排序是不稳定的，因为如果有相同的数的话是可以改变原来次序的，平均时间复杂度：O(n²)。 插入排序​ 看到这个方法，我的第一反应便是抓扑克牌，原理和抓牌原理一样，即，左手上的牌是已经排好序了的，将左手上的牌依次和抓到的牌比较，如果大于抓到的牌便把这张牌左移，然后插入抓到的牌。函数代码如下: 1234567891011121314void InsertionSort(int A[],int n)&#123; for(int i = 1;i &lt; n;i++) &#123; int get = A[i]; int j = i - 1; while(j &gt;= 0 &amp;&amp; A[j] &gt; get) &#123; A[j + 1] = A[j]; j--; &#125; A[j + 1] = get; &#125;&#125; 相同的牌不影响顺序，插入排序是稳定的，平均时间复杂度：O(n²)。 快速排序​ 快速排序基于一种二分的思想，即以一个数为基准数，不断将数组二分，最终当所有基准数都归位后，排序也就完成了。快速排序之所以较快，是因为每次交换都是跳跃式的。函数代码如下：(千万注意下标是从0开始的 ) 12345678910111213141516171819202122void quicksort(int arr[],int left,int right)&#123; if(left &gt; right) return; int i,j,temp; temp = arr[left];//temp为基准数 i = left; j = right; while(i != j) &#123; //基准数在左边，所以要从右边开始找 while(arr[j] &gt;= temp &amp;&amp; i &lt; j)j--; //再从左往右找 while(arr[i] &lt;= temp &amp;&amp; i &lt; j)i++; //如果没有相遇。就交换 if(i &lt; j) Swap(arr,i,j); &#125; arr[left] = arr[i]; arr[i] = temp; quicksort(arr,left,i - 1);//继续处理左边 quicksort(arr,i + 1,right);//继续处理右边 &#125; 这两个等我会了再写233333 归并排序归并排序的原理是分治法，简单点说就是把一个序列拆成多个子序列，将子序列排好序后，再将其合并为一个序列。归并排序的效率也比较可观，达到了o(NlogN)。 1234567891011121314void mergeArray(int a[], int first, int mid, int last, int temp[])&#123; int i = first, j = mid + 1; int m = mid, n = last, k = 0; while(i &lt;= m &amp;&amp; j &lt;= n) &#123; if(a[i] &lt;= a[j]) temp[k++] = a[i++]; else temp[k++] = a[j++]; &#125; while(i &lt;= m) temp[k++] = a[i++]; while(j &lt;= n) temp[k++] = a[j++]; for(i = 0;i &lt; k;i++) a[first + i] = temp[i];&#125; 12345678910void mergeSort(int a[], int first, int last, int temp[])&#123; if(first &lt; last) &#123; int mid = (first + last) / 2; mergeSort(a, first, mid, temp);//处理左边 mergeSort(a, mid + 1,last, temp);//处理右边 mergeArray(a, first, mid, last, temp);//合并 &#125;&#125; 堆排序]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Sublime配置成python环境]]></title>
    <url>%2F2017%2F07%2F19%2F%E5%B0%86Sublime%E9%85%8D%E7%BD%AE%E6%88%90pythona%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[刚开始学python的时候，用的是python自带的IDLE.bat，总感觉太简陋，字体很小，也没有自动补全的东西，之前有段时间在写网页玩，一直用着Sublime text 3——这款轻量而且扩展性很强而且颜值高的编辑器，想着在St3上写python岂不是美滋滋了，于是搜索了一波，最后发现了St3直接提供了一个强大的插件——Anaconda，可以将St3打造成一个python IDE。 下载好python和Sublime text 3打开安装包管理工具（这个还不会的小白请自行百度，不太好截图），输入Install Package，然后输入Anaconda，回车安装保存文件名为.py后缀后就会发现写python会有高亮和自动补全了。可能遇到的问题 用ST3一打开python文件就有这个问题，发现后面写着try to set the ‘swallow_startup_errors’ to ‘true’ 然后发现这个json的设置居然是在SublimeREPL里面而不是Anaconda。。。 好吧不管了 先试试再说 进入Preferences–package settings–&gt;SublimeRPEL–&gt;settings user 键入： “swallow_startup_errors”: true, 保存后重启。咦，那个报错好像真的没了诶。于是开心的在几个py文件直接切换试试，然后发现切太快这个错误还是有可能出来，具体原因不详，不过比之前好多了 还有可能遇到这种情况： 大白框和小白点，猜想可能是因为python的缩进？ （你自己喜欢的话就没关系，不用改了） 我个人看着很不舒服，搜索后发现这个是默认开启的功能，点开Preferences-Pacakage Settings-Anaconda-Settings user 加上如下一句： 即可解决这个问题. 然后就可以愉快地用St3写Python了。（ctrl + B 即可运行） 嗯，故事就这么结束了么？当然没有 然后就遇到了第一个坑：Input无效 Sublime似乎无法完成input这种交互式命令 解决方法请参考此链接]]></content>
      <categories>
        <category>环境</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Sublime</tag>
      </tags>
  </entry>
</search>
