<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第四届世安杯酱油记]]></title>
    <url>%2F2017%2F10%2F08%2F%E7%AC%AC%E5%9B%9B%E5%B1%8A%E4%B8%96%E5%AE%89%E6%9D%AF%E9%85%B1%E6%B2%B9%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[​ 给21号的校赛练练手。 @kurum 主攻web，@a-lie-Z 主要负责misc和steganography，我试了下有思路的题，下面是我找到的flag（emmm，为什么不说是我做出来的题呢？）@a-lie-Z 做的题：传送门 ctf入门级题目​ 戳进去看发现有源码，放一下php代码： 12345678if (isset ($_GET['password'])) &#123; if (ereg ("^[a-zA-Z0-9]+$", $_GET['password']) === FALSE) echo '&lt;p class="alert"&gt;You password must be alphanumeric&lt;/p&gt;'; else if (strpos ($_GET['password'], '--') !== FALSE) die($flag); else echo '&lt;p class="alert"&gt;Invalid password&lt;/p&gt;';&#125; ​ 第一个if要求password 必须为数字和字母，第二个if要求子串中得有”–”，看上去有点矛盾，而输入数组会使ereg函数发生错误，返回false，因此可以通过第一个if，而字符数组同样会使得strpos函数出错返回Null，与严格的FALSE不符，这样就能通过第二个if判断。于是构造url 为 http://ctf1.shiyanbar.com/shian-rao/password[]=xxx 来绕过。 曲奇饼​ 嗯，名字貌似提示需要构造cookie。打开题目链接后发现url有点奇怪：http://ctf1.shiyanbar.com/shian-quqi/index.php?line=&amp;file=a2V5LnR4dA== ，后面那个file=a2V5LnR4dA== 有点像base64，解密一下后发现是key.txt，然而这里并没有什么，改了line参数也没什么鸟用。于是考虑获取真正index.php，去掉后面的参数后发现依旧会重定向到这里，然后考虑加参数，受key.txt的提示，我把index.php用base64加密后传了上去 http://ctf1.shiyanbar.com/shian-quqi/index.php?line=&amp;file=aW5kZXgucGhw ，发现什么都没有，试着把line改成1，输出了 error_reporting(0); 换成2试试，$file=base64_decode(isset($_GET[&#39;file&#39;])?$_GET[&#39;file&#39;]:&quot;&quot;); 再莫多说，写python，获取完整的php代码，如下： 1234567891011121314151617error_reporting(0);$file=base64_decode(isset($_GET['file'])?$_GET['file']:"");$line=isset($_GET['line'])?intval($_GET['line']):0;if($file=='') header("location:index.php?line=&amp;file=a2V5LnR4dA==");$file_list = array('0' =&gt;'key.txt','1' =&gt;'index.php',);if(isset($_COOKIE['key']) &amp;&amp; $_COOKIE['key']=='li_lr_480')&#123;$file_list[2]='thisis_flag.php';&#125;if(in_array($file, $file_list))&#123;$fa = file($file);echo $fa[$line];&#125; ​ 嗯，果然和名称暗示的一样，构造cookie，传入&#39;key&#39;=&#39;li_lr_480&#39;，再把$file改成thisis_flag.php的base64形式即可。python代码如下： 1234567import requests cookie = &#123;'key':'li_lr_480'&#125;for i in range(50): url="http://ctf1.shiyanbar.com/shian-quqi/index.php?line="+str(i)+"&amp;file=dGhpc2lzX2ZsYWcucGhw" ans = requests.get(url, cookies=cookie) print ans.text 拿到flag。 console​ 用PEiD打开后发现是.net框架，用ILSpy，打开，把重要文件的代码整合，核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556internal class MM&#123; private static int C(int A_0, int A_1) &#123; return (new int[] &#123; 2, 3, 5, 7, 11, 13, 0x11, 0x13, 0x17, 0x1d, 0x1f, 0x25, 0x29, 0x2b, 0x2f, 0x35,0x3b, 0x3d, 0x43, 0x47, 0x49, 0x4f, 0x53, 0x59, 0x61, 0x65, 0x67, 0x6b, 0x6d, 0x71 &#125;)[A_1] ^ A_0; &#125; private static string flag(string A_0) &#123; byte[] bytes = Encoding.ASCII.GetBytes(A_0); return &quot;flag&#123;&quot; + BitConverter.ToString(new MD5CryptoServiceProvider().ComputeHash(bytes)).Replace(&quot;-&quot;, &quot;&quot;) + &quot;&#125;&quot;; &#125; private static void count(string A_0, int A_1, ref string A_2) &#123; int num = 0; if (0 &lt; A_0.Length) &#123; do &#123; char c = A_0[num]; int num2 = 1; do &#123; c = Convert.ToChar(MM.C(Convert.ToInt32(c), num2)); num2++; &#125; while (num2 &lt; 15); A_2 += c; num++; &#125; while (num &lt; A_0.Length); &#125; A_2 = MM.flag(A_2); &#125; private static void main(string[] A_0) &#123; string b = null; string value = string.Format(&quot;&#123;0&#125;&quot;, DateTime.Now.Hour + 1); string a_ = &quot;CreateByTenshine&quot;; MM.count(a_, Convert.ToInt32(value), ref b); string a = Console.ReadLine(); if (a == b) &#123; Console.WriteLine(&quot;u got it!&quot;); Console.ReadKey(true); &#125; else &#123; Console.Write(&quot;wrong&quot;); &#125; Console.ReadKey(true); &#125;&#125; c#脚本 123456789101112131415161718using System;using System.Security.Cryptography;using System.Text; namespace ConsoleApplication1&#123; class Program &#123; static void Main(string[] args) &#123; string a = &quot;[j&#125;yl&#125;ZaL&#125;vkpqv&#125;&quot;; byte[] bytes = Encoding.ASCII.GetBytes(a); string b = &quot;flag&#123;&quot; + BitConverter.ToString(new MD5CryptoServiceProvider().ComputeHash(bytes)).Replace(&quot;-&quot;, &quot;&quot;) + &quot;&#125;&quot;; Console.WriteLine(b); Console.ReadLine(); &#125; &#125;&#125; ​ 本机没配c#环境，随便找个在线网站跑就OK。 low​ 打开后发现是一张图片，看通道没什么反应，分析也没什么头绪。但从名字来看，可能是要把低位的0都获取出来，然后拼图，用python跑一下试试（matlab本机没有环境，线上貌似没法上传图片）。 123456789101112131415161718192021import Imagedef foo(): im=Iamge.open('low.bmp') im2=im.copy() pix=im2.load() width,height=im2.size for x in xrange(0,width): for y in xrange(0,height): #LSB if pix[x,y]&amp;0x1==0: pix[x,y]=0 #黑 else: pix[x,y]=255 im2.show() passif __name__ == '__main__': foo() 貌似成功了，可以得到一张图片，左上角有一个二维码，反色扫一下即可。 珍妮的QQ号​ 下载是个压缩包，解压后就是一个txt文件，里面的内容： 1珍妮换了一个新的QQ号码，原来的号码和新的号码都是5位靓号哦；其次，新的号码是原来号码的4倍，并且原来的号码倒着写正好是新的号码，请问，新号码是多少，新号码即为key。 很简单的编程题，直接上c代码。 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt;int main ()&#123; int n; for(n = 10001;n &lt;= 25000;n++) &#123; if((n * 4) == MyFunction(n)) printf(" %d ",n * 4); &#125; return 0;&#125;int MyFunction(int number) &#123; int k, fig=0, i, j, num, sum=0; k=number; while(k!=0) &#123; ++fig; //fig用来保存输入的数的位数 k /= 10; &#125; for(i=0; i&lt;fig; i++) &#123; num = number % 10; //求余数 for(j=0; j&lt;fig-i-1; j++) num = num*10; //余数乘上所对应的的10的次方 sum = sum + num; number /= 10; &#125; return sum;&#125; 跑出来结果是87912. RSA​ 打开是一个message.txt文件，里面写着： 12c = 2044619806634581710230401748541393297937319n = 92164540447138944597127069158431585971338721360079328713704210939368383094265948407248342716209676429509660101179587761913570951794712775006017595393099131542462929920832865544705879355440749903797967940767833598657143883346150948256232023103001435628434505839331854097791025034667912357133996133877280328143 RSA的具体原理还不是很懂，大概说的就是公钥私钥什么的，把c复制到google然后。。。就搜出了flag。。。（→_→这线上赛的题也是够了，然后发现，其他有些题也能搜出来，无语了。。。） 贴一下大佬的话： Nietypowa rzecz jest taka, ze wykładnik szyfrujący nie jest podany i c jest bardzo małe w porównaniu do n. To sugeruje że może e było tak małe że m^e nie przekroczyło n, albo tylko nieznacznie. Żeby to sprawdzić postanowiliśmy policzyć k-te pierwiastki całkowite dla c w poszukiwaniu e: 以及求解代码： 1234567891011121314import gmpy2from src.crypto_commons.generic import long_to_bytesdef main(): c = 2044619806634581710230401748541393297937319 n = 92164540447138944597127069158431585971338721360079328713704210939368383094265948407248342716209676429509660101179587761913570951794712775006017595393099131542462929920832865544705879355440749903797967940767833598657143883346150948256232023103001435628434505839331854097791025034667912357133996133877280328143 for i in range(2, 10): root = gmpy2.iroot(c, i)[0] if root**i == c: print(i, long_to_bytes(root))main() 然后flag为so_low ，呃，RSA很low吗？ 喵喵喵？ （比赛虽然有点水，但自己更水，ctf还是适合作为普及知识面的竞赛）]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>CTF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客加https && google收录]]></title>
    <url>%2F2017%2F10%2F03%2Fhexo%E5%8D%9A%E5%AE%A2%E5%8A%A0https%20%26%26%20google%E6%94%B6%E5%BD%95%2F</url>
    <content type="text"><![CDATA[​ 讲讲上https和让谷歌收录个人博客的过程。 前言​ 戳友链的时候发现好多博客都上了全站https，而且https已经是大势所趋，特别是谷歌，对https的网站收录极其友好(至于百度，emmmmm，我已经放弃它了)，而谷歌搜索是当今最好用的引擎，可惜在中国大陆无法正常访问，需要科学上网，方法很多，免费的不太稳有限制而且需要多折腾，花钱的省事而且稳定，在这我安利一下 ACGPOWER，十分好用，而且只要6块一个月。(PS：自从用过了这个，我的地址栏默认搜索引擎换成了Google。。。妈妈再也不用担心我科学上网不稳了)。好了废话就说到这，下面是正文。 https​ 关于https，全称为：Hypertext Transfer Protocol Secure，比http多了个S，顾名思义，多了一层保护，即TLS/SSL协议，早些年，SSL证书基本上是要花大价钱买的，很难看到免费的，而现在随着https越来越普及，免费的SSL证书也很多，可以自行选择适合自己的SSL证书。全站的https网站会将要传递的信息进行加密后传输，可以避免信息窃听、信息篡改、信息劫持等问题。具体原理可以参考这里。 Cloudflare介绍与https正式部署​ Cloudflare 是一家可以给全球用户提供网站访问加速、DNS解析、阻止网站攻击等众多服务的公司，具体介绍自行去官网。那么我要的就是Cloudflare的SSL服务了，方法很简单。大概就是一下几个步骤：(讲真Cloudflare给的引导过程挺清楚的) 1、去Cloudflare注册，添加自己的个人网站，扫描过程大概1分钟。 2、验证域名所有权后，等待status变成active状态。 3、拿到Cloudflare的Domin Name Server后，去域名提供商那修改自己域名的Domin Name Sever。(去那个域名列表那戳管理就行) 4、改完DNS后点击Crypto，在SSL那选择Flexible，如果自己有证书的话可以选择Full(用github pages的貌似无法再部署SSL)，大概区别如下： 具体的区别自己戳旁边的链接，都有很详细的介绍，非常良心的一家网站诶。其他的话，开启强制HTTPS以及Automatic HTTPS Rewrites(如果资源路径支持https，那么会自行跳转成https)这两个就可以。 ​ 以上步骤全部完成后，如果你没有引用http的资源(即混合引用)，是可以看到小绿锁的。 PS:为此，我把图床从七牛搬到了新浪博客。。。。 PPS：新浪博客的图床真的挺好用的。 Google收录1、首先在根目录下键入以下命令安装sitemap插件 npm install hexo-generator-sitemap --save 2、然后在站点文件(不是主题文件哦)增加以下内容(注意缩进) 123# 自动生成sitemapsitemap: path: sitemap.xml hexo g以下就能在public里面看到sitemap.xml了。 3、在source下增加robots.txt 123456789101112131415# hexo robots.txtUser-agent: *Allow: /Allow: /archives/Allow: /categories/Allow: /tags/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: https://primykq.top/sitemap.xml 把网址改成自己的。 4、进入Google Search Console，没有谷歌账号的注册一个。添加网站，把自己的个人网站输进去，这里要验证域名所有权。我选择了文件验证，把下的文件放在站点的source目录下，注意：不要编译 不要编译 不要编译 直接hexo d 把这份文件上传到服务器中，编译的话hexo会自动给你加上一大串html代码。 5、验证完了以后测试一下robots.txt协议。 6、提交sitemap，输入sitemap.xml就行。 7、抓取那一栏可以加上标签栏或者你觉得最容易被搜到的page，不加的话默认抓取首页，填完url后点击抓取，然后就点击“提交至索引”。注意了，这里可能要进行人机验证，我不得不吐槽一下这google的人机验证，比12306的还要骚，一堆图片，让你选有汽车呀路标呀什么的图片，然后，你点中以后，这个图会消失，然后出新的图，如果又有汽车或者路标等目标图片，你需要继续点，一直点到没有指定的图片为止，如果你提交验证后失败，请直接ctrl + r刷新后重来，我感觉后面是个无底洞，反正我在后面没成功过，你感兴趣的话可以试试。上两张图好了。 提交完索引后就大功告成了。大概过了那么一两分钟，我去google搜索那输入了primykq的博客，妈耶，马上就出来了，这收录速度真牛。 多说一句​ 有人可能会说，咋没有提交到百度的教程，emmmm，百度，我卡死在域名所有权验证那里，怎么都过不去，后来想想，还得双线部署到coding(因为github 禁了百度的爬虫)，暂时懒得折腾了，用了独立域名后访问速度也还过得去，大不了再压缩一下资源文件。(后来才知道，github下的二级域名可以直接强制https，可惜我还是喜欢个人的顶级域名。)]]></content>
      <categories>
        <category>环境</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>https</tag>
        <tag>Cloudflare</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTF传送门]]></title>
    <url>%2F2017%2F09%2F29%2FCTF%E4%BC%A0%E9%80%81%E9%97%A8%2F</url>
    <content type="text"><![CDATA[​ 看到不错的分享，直接码在自己博客上比较方便(其实是chrome不想加那么多的书签QAQ，CTF方面的学习资料会继续在这补)。不过有的貌似挂了，比如我挺喜欢的IDF实验室 http://edu.gooann.com/ 谷安网校http://www.jikexueyuan.com/ 极客学院http://www.hetianlab.com/ 合天http://www.moonsos.com/ 米安网http://www.ichunqiu.com/ i 春秋http://www.honyaedu.com/ -红亚http://www.baimaoxueyuan.com/ 白帽学院http://www.simplexue.com/ctf/index 西普学院http://www.imooc.com/course/list 慕课http://www.secbox.cn/ 安全盒子http://www.freebuf.com/ freebufhttp://bobao.360.cn/ 360安全播报http://www.wooyun.org/ 乌云http://drops.wooyun.org/ 乌云知识库http://wiki.wooyun.org/ WooYun WiKihttps://www.91ri.org/ 91rihttps://www.t00ls[.NET](http://lib.csdn.net/base/dotnet)/toolshttp://www.ijiandao.com/ 爱尖刀http://www.secwk.com/article/index.html 威客众测http://bluereader.org/ 深蓝阅读http://www.shentou.org/ 黑客安全军火库http://netsecurity.51cto.com/ 51ctohttp://security.csdn.net/ csdnhttp://www.80sec.com/ 80sec teamhttps://security.alibaba.com/blog.htm?spm=0.0.0.0.knOqaI 阿里巴巴安全响应中心http://security.tencent.com/index.[PHP](http://lib.csdn.net/base/php)/blog 腾讯安全应急响应中心 博客http://security.360.cn/blog 360安全应急响应中心 博客http://sec.baidu.com/index.php?research/list 百度安全应急响应中心 博客 博客推荐http://security.tencent.com/index.php/bloghttp://217.logdown.com/ 217http://www.blue-lotus.net blue-lotus 蓝莲花http://blog.0ops.net/ 0opshttp://le4f.net/ e4fhttp://www.programlife.net/ 代码疯子http://www.hackdog.me/ redrain’bloghttp://www.syjzwjj.com/ 俊杰http://syclover.sinaapp.com/ 三叶草安全小组http://appleu0.sinaapp.com/ appleU0大大http://bl4ck.in/ tomato表哥http://www.sco4x0.com/ 4叔叔http://laterain.sinaapp.com/ 白神http://0nly3nd.sinaapp.com/ 0nly3ndhttp://hijacks.in/ LateRain’bloghttp://www.waitalone.cn/ 独自等待http://evilcos.me/ 余弦http://www.moonsec.com/ 暗月http://www.cnblogs.com/xuanhun/ 玄魂https://www.leavesongs.com/ 离别歌http://huaidan.org/ 鬼仔http://www.03sec.com/ sky的自留地http://joychou.org/ jc老师http://www.unhonker.com/ 90’s bloghttp://www1.taosay.net/ 道哥的黑板报http://blog.knownsec.com/ 知道创于http://www.sadk.org/ 焠安http://www.cnseay.com/ seay’bloghttp://blog.aptsec.net/ AptSec Teamhttp://lcx.cc/ 网络安全研究中心http://www.kali.org.cn/ kali中文网http://xiao106347.blog.163.com/ xiao106347 kali折腾更多大家推荐 渗透:http://www.wooyun.org/ 乌云http://bbs.blackbap.org/ 习科http://www.1937cn.net/ 1937http://forum.cnsec.org/ 暗组http://www.k33nteam.org/ keen teamhttp://forum.eviloctal.com/ 邪恶八进制http://www.evil0x.com/ 邪恶十六进制http://www.myhack58.com/ 黑吧安全吧http://www.cnhonkerarmy.com/ 中国红客 红盟http://www.chinahacker.com/ 中国黑客联盟http://www.hxhack.com/ 华夏黑客联盟http://www.heikexiehui.com/ 中国黑客协会官网http://www.hackbase.com/ 黑基http://www.2cto.com/ 红黑联盟http://bbs.2cto.com/ 红黑联盟论坛http://www.hackwd.com/http://www.heishou.com.cn/ 黑手安全网https://www.sitedirsec.com/ 非安全中国网http://www.zatokasztuki.com/ 学生技术联盟 逆向http://www.52pojie.cn/ 吾爱破解http://bbs.pediy.com/ 看雪论坛一蓑烟雨貌似被关了，逆向方面了解不多，问问表哥们吧( ?? ω ?? )y 编程http://www.he11oworld.com/ hello wordhttp://www.w3school.com.cn/ w3schoolhttp://www.runoob.com/ 菜鸟http://www.51zxw.net/https://github.com/http://navisec-[Git](http://lib.csdn.net/base/git).qiniudn.com/http://c.biancheng.net/cpp/http://www.liaoxuefeng.com/http://www.php100.com/https://ruby-china.org/wikihttp://bbs.csdn.net/forums/[Java](http://lib.csdn.net/base/javaee)/http://outofmemory.cn/tutorial/ 书籍http://zhuanlan.zhihu.com/Evi1m0/19706178 Evi1m0: 书籍推荐http://www.douban.com/doulist/3339701/ 信息安全必读书单http://www.douban.com/doulist/1363865/ 信息安全经典书籍http://www.zhihu.com/question/21390646http://my.oschina.net/bluefly/blog/335409?utm_source=tuicool&amp;utm_medium=referral Web安全核心书单连载《安全参考》http://www.douban.com/group/topic/72383272/ (2013年第一期–2015年第一期)全集《书安》(更新中)http://www.secbox.cn/hacker/8205.html 书安SecBook 第一期《icloud iOS安全大揭秘》http://www.secbox.cn/hacker/7366.html 书安SecBook 第二期《信息安全攻防赛》渗透实战文章可以看看里的杨凡(http://blog.sina.com.cn/s/articlelist_1758675673_4_1.html)和法克文章（http://pan.baidu.com/share/link?shareid=249629&amp;uk=2198816663） 工具(黑软有分风险，最好在虚拟机里搞)ftp://222.18.158.199/(校园网内网可以访问，不仅仅只有工具哦，有许多总结，各方面的)http://forum.cnsec.org/thread-94330-1-1.html 2015暗组工具包（渗透）http://bbs.secbox.cn/thread-196-1-1.html 2015_安全盒子工具包http://www.secbox.cn/hacker/tools/3552.html 法客论坛2015工具包-第三版http://www.52pojie.cn/forum.php?mod=viewthread&amp;tid=388015 吾爱破解工具包 2015/7/22（逆向）http://down.52pojie.cn/ 爱盘 – 在线破解工具包，教程，http://www.52pojie.cn/thread-341238-1-1.html 吾爱破解论坛专用破解虚拟机ctf常用工具包请看http://tieba.baidu.com/p/3933947157里面群文件 其他http://www.zhihu.com/topic/19558642 黑客知乎话题http://www.zhihu.com/topic/20011446 ctf知乎话题http://www.zhihu.com/topic/19561983 信息安全知乎话题http://zhuanlan.zhihu.com/evilcos/19961466 余弦知乎专栏 CTF方面http://blog.idf.cn/2015/02/ctf-field-guide/http://tieba.baidu.com/p/3933947157 ctf大全https://ctftime.org/event/list/upcoming 各种CTF赛事预告(ps:国内各个高校或企业举办的比赛请进http://tieba.baidu.com/p/3933947157里面的群) 平时ctf练习 ctf逆向:http://reversing.kr/http://pwnable.kr/http://exploit-exercises.com/http://overthewire.orghttp://security.cs.rpi.edu/courses/binexp-spring2015/ bin 干货区http://www.52pojie.cn/forum-67-1.html 『2014CrackMe大赛』 SQL:https://github.com/Audi-1/sqli-labshttp://redtiger.labs.overthewire.org/ ctf XSS:http://prompt.ml/http://xss.pkav.net/xss/http://www.doscn.org/xss/http://xss-quiz.int21h.jp/http://escape.alf.nu/ ctf综合练习： http://hackinglab.cn/ 网络信息安全攻防学习平台http://captf.com/ ctf题目http://cafebabe.cc/nazo/ 脑洞开发，与ctf只有那么一点关系，有85关了( ?? ω ?? )yhttp://1111.segmentfault.com/ 光棍节程序员闯关秀http://www.helloisa.com/test/http://www.fj543.com/hack/ 黑客丛林之旅http://monyer.com/game/game1// 梦之光芒的小游戏http://oj.xctf.org.cn/ XCTF_OJ练习平台http://hackgame.blackbap.org/ 习科黑客游戏http://ctf.3sec.cn/ Jlu.CTFhttp://www.baimaoxueyuan.com/ctf 白帽学院ctf挑战赛http://www.ichunqiu.com/tiaozhans i春秋ctf挑战http://ctf.idf.cn/ idf 实验室http://ctf.moonsos.com/pentest/index.php 米安网ctfhttp://www.hetianlab.com/CTFrace.html 合天ctfhttp://www.shiyanbar.com/ctf/index 实验吧(前名西普学院）http://hkyx.myhack58.com/ 黑吧安全网-红客闯关游戏http://202.108.211.5/ 实训竞赛系统 国外比较好的几个综合练习平台http://www.wechall.nethttp://insight-labs.org/http://wargame.kr/http://canyouhack.it/http://hackit.sinaapp.com/http://webhacking.kr/http://fun.coolshell.cn/http://ringzer0team.com/challengehttps://backdoor.sdslabs.co/http://smashthestack.org/ 漏洞利用练习网站 ctf writeup(WP):直接百度:writeup(一边看一边总结，有道笔记，印象笔记什么的，最好能复现)另外乌云和360安全播报上有些WPhttp://drops.wooyun.org/?s=writeup&amp;submit=%E6%90%9C%E7%B4%A2http://bobao.360.cn/ctf/https://github.com/ctfs/ 各种 writeuphttp://sec.yka.me/ CTF Writeup Summaryhttps://ctf-team.vulnhub.com/ Write Ups ​ 原地址： 链接1 A方向： RE for BeginnersIDA Pro权威指南揭秘家庭路由器0day漏洞挖掘技术自己定操作系统黑客攻防技术宝典：系统实战篇 有各种系统的逆向讲解 B方向： Web应用安全权威指南 最推荐小白，宏观web安全Web前端黑客技术揭秘黑客秘籍——渗透测试实用指南黑客攻防技术宝典 web实战篇 web安全的所有核心基础点，有挑战性，最常规，最全，学好会直线上升代码审计：企业级web代码安全架构 入门—-从基础题目出发（推荐资源）： http://ctf.idf.cn !!!首推 idf实验室：题目非常基础，只1个点www.ichunqiu.com 有线下决赛题目复现http://oj.xctf.org.cn/xctf 题库网站，历年题，练习场，比较难www.wechall.net/challs !!!!!!非常入门的国外ctf题库，很多国内都是从这里刷题成长起来的http://canyouhack.it/ 国外，入门，有移动安全https://microcorruption.com/login A方向 密码，逆向酷炫游戏代http：//smashthestack.org A方向，简洁，国外，wargames，过关http://overthewire.ofg/wargames/ ！！！！推荐A方向 国内资料多，老牌wargamehttps：//exploit-exercises.com A方向 老牌wargame，国内资料多http://pawnable.kr/play.php pwn类游乐场，不到100题http://ctf.moonsoscom/pentest/index.php B方向 米安的Web漏洞靶场，基础，核心知识点http：//prompt.ml/0 B方向 国外的xss测试http://redtiger.labs.overthewire.org/ B方向 国外sql注入挑战网站，10关，过关的形式 不同的注入，循序渐近地练习 工具：https://github.com/truongkma/ctf-toolshttps://github.com/Plkachu/v0lthttps://github.com/zardus/ctf-toolshttps://github.com/TUCTF/Tools 这是原链接 ：链接2。 ​]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>CTF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT创新区招新酱油记]]></title>
    <url>%2F2017%2F09%2F26%2FIT%E5%88%9B%E6%96%B0%E5%8C%BA%E6%8B%9B%E6%96%B0%E9%85%B1%E6%B2%B9%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[​ 一直觉得自己很菜，但具体程度不知道，直到…….南航IT创新区招新，本着为以后的技术面积累经验的目的，就去报名参加了。 虽然通知上说IT创新区的合作组织比如纸飞机技术部、微软技术部、科协宣企等技术部门可以不参加面试，后期考核一下合格后直接转入，但别人认可的是技术部的水平，感觉自己离那个水平有点远，所以还是老老实实去走流程。 ​ 先笔试，前面有数据结构与算法、Web大前端、网络后端、逻辑思维能力这几个板块，有多项选择题，填空题。然后后面就是这几个板块的面试题，自行准备有把握或者有思路的题，做完前面会做的就可以去排队面试了。有印象的题不多，因为大部分完全不会，稍微整理一下还有点印象的题。 逻辑思维题​ 填空题里有好几个印象挺深的题，先来看第一个吧。 火车运煤问题​ 题目是这样的： 你是山西的一个煤老板，你在矿区开采了有3000吨煤需要运送到市场上去卖，从你的矿区到市场有1000公里，你手里有一列烧煤的火车，这个火车最多只能装1000吨煤，且其能耗比较大——每一公里需要耗一吨煤。请问最后能运到集市上卖多少吨煤？ ​ 当时脑子里一片混乱，被前端的知识点和数据结构的东西弄的有点晕，心里想着反正答案不是零，但要分段运的话。。。在草稿本上画了画图，算来算去有点不清晰，懒得算了。后来去网上搜索了一下，才发现这是有名的”火车运煤问题“，(此处更新一条消息。我正写到这里，IT创新区发来了机考的通知。。。呃，一面要求这么低的吗。。。。还以为被刷定了)看了看别人的解法，再想了会，其实也不是很难，但要在短时间内做很多这样的题目，我就感觉有点变态了。 ​ 3000吨煤初步设想肯定是要运3次的，因为一次最多只能运1000吨，大概思路就是：在中间设两个暂存点1和2，第一次运到1，留下返程的煤然后回去，装满1000后到1处将煤补满至1000，到2位置，然后卸掉一些煤，留下返回到1的煤，到1后再装上回到矿区的煤，接着再把最后的1000吨煤全装上，到1时将这里剩下的煤都装上（应该是刚好凑成1000的），然后到2后再装上2处剩下的煤（同理应该恰好装满），最后运到市区。方程最后算出来的结果应该是533.3t，有兴趣的自己解下方程试试。其实这里有一个 突破点，每运1公里是要耗1吨煤的，所以一开始就能确定只需要从起点运3次，即每次都装满1000t。 这个解法直接把此问题转为了线性规划，需要一定的数学能力，不是我等数学渣渣可以轻易做到的。。。。 海盗分金​ 这个问题是经济学上有名的模型（甚至还给它建了百度百科。。。），以前没怎么思考过，还好上次离散课上老师稍微提了一下，就说了一下逆推法，不过要注意的是这个题目说的是方案必须大于半数才能通过。找到了方法其实并不难，如果最后剩下4号和5号，5号投反对票，就可以独得金币，所以3号只需要给4号一点金币让他投赞成票就行了(其实就算3号不给4号，他也只能投赞成票，不然他就得死)，然后2号就只需要给4号和5号一点点金币就可以了，同理，1号只需要给3号一点好处，然后4号或5号选一个给一点就行了，这样就能保证自己的利益最大化。重点就是后面的那个人一定会投反对票，以及，逆推 前端后端canvas和svg的区别​ svg，这玩意我听说的很少，查了下后才知道，svg年代久远，已经十几年了，可以自定义标签或属性，用来描述二维图形，canvas倒是看过一些大佬们写的博客，有些许了解。 ​ 从功能上来讲，canvas是画布，画出来的图形为标量图形，所以，canvas可以直接引入jpg或者png类的图片，在实际开发中，canvas也很受欢迎，而且现在canvas的技术也比较成熟了。而svg则是用来描绘矢量图的，所以不支持直接引入普通的图片，但svg的矢量图不会失真，所以可以用来做一些动态的小图标，由于svg的图片不会失真，很适合用来做地图，不管怎么放大都很清晰，比如百度地图。 ​ 在技术层面，canvas绘制的图片是不能被引擎抓取的，一般用javascript，而svg更多是通过标签来实现的，并且支持事件绑定。 javascript中this的用法​ 我印象中，this就是指向当前函数的直接调用者来着，别的没了。。。呃，查了一下，简单点说就是这样： 有对象就指向调用对象 没调用对象就指向全局对象 用new构造就指向新对象 通过 apply 或 call 或 bind 来改变 this 的所指。 具体解释 http/https tcp/udp socket​ 首先说下tcp和udp，两者都是超文本传输协议，属于传输层，但区别是，在IP环境下TCP比UDP传输数据的可靠性更强，所以tcp/ip协议对应的是数据可靠性更高的应用，tcp协议支持的应用层协议主要有：Telent、FTP、SMTP等，而UDP支持的应用层协议主要有：NFS(网络文件系统)、SNMP(简单网络管理协议)、DNS(主域名称系统)、TFTP(通用文件传输协议)等。 ​ socket简单点说就是网络上的两个程序通过一个双向的通信连接实现数据的交换，连接的一端就称为一个socket。而建立网络通信连接至少要一对端口号(socket)，其本质就是API，对TCP/IP的封装，而TCP/IP也要提供可供程序猿做网络开发所用的借口，即socket编程接口。 ​ http与https的区别就在于有无一层加密模块(SSL)，http传输的数据都是明文，很不安全，于是诞生了https，一开始用的版本是SSL，但现在基本上换成了TLS。有一个问题经常会被问到：在浏览器地址栏输入https开头的URL，到整个网页界面显示完毕，发生了什么？其实问的就是https的原理。上个图 ​ 1.客户端输入URL后，发起了一个https的请求，把自身支持的一系列Cipoer Suite(密钥算法套件)发给服务端 ​ 2.服务端接受到客户端所有的Cipher后与自身支持的对比，如果不支持就断开连接，支持的话就从中选出一种加密算法，以证书的形式返回给客户端，证书中会包含公钥、颁证的机构、网址、失效日期等等。 ​ 3、客户端收到服务器响应后会做的事： ​ 3.1 验证证书的合法性。即机构是否合法、是否过期，证书中包含的网站地址是否与正在访问的的地址一致等，证书验证通过后，在浏览器的地址栏会加上一把小锁(Chrome的亲测有效) ​ 3.2 生成随机密码。如果证书验证通过，或者用户接受不了授信的证书，此时浏览器会生成一串随机数，然后用证书中的公钥加密。 ​ 3.3 HASH握手信息。用最开始约定好的HASH方式，把握手消息取HASH值，然后用随机数加密“握手消息+握手消息HASH值(签名)”并一起发给服务端。这里之所以要取握手消息的HASH值，是为了把握手消息做一个签名，用于验证握手消息在传输过程中有没有被修改过。 ​ 4、 服务端拿到客户端传来的密文，用自己的私钥来解密握手消息去除随机数密码，再用随机密码解密握手消息与HASH值，并与传过来的HASH值做对比确认是否一致。然后用随机密码加密一段握手消息(握手消息+握手消息的HASH值)给客户端。 ​ 5、 客户端用随机数解密并计算握手消息的HASH，如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码并利用对称加密算法进行加密，因为这串密钥只有客户端和服务端知道，即使在中间被拦截也无法破解。 非对称加密算法：RSA，DSA/DSS，客户端与服务端相互验证的过程是对称加密 对称加密算法：AES，RC4，3DES 验证通过后，以随机数作为密钥，就是对称加密 HASH算法： MD5，SHA1，SHA256 确认握手消息没有被篡改时 后来​ 今天按照通知去机考，然后被PM发现是技术部的(但实际上我比技术部的平均水平差太多，所以不好意思说自己是技术部的…)最后提前走了，因为想做的板块自身基础的确不好，被告知等技术成熟了，随时接受内推。。。。(被小小地照顾了一下自尊心)]]></content>
      <categories>
        <category>开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫系统学习笔记]]></title>
    <url>%2F2017%2F09%2F16%2Fpython%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[在中国大学MOOC那发现了python的爬虫专题系列，可以说是非常非常开心了 Requests库入门​ 在前两次的爬虫体验里面已经用到了这个牛X的库，现在开始对它进行系统点的学习（emmm，个人不是很喜欢看文档，喜欢视频），嵩天老师讲的超级好，之前python入门也看过一点他的视频，感觉超级赞。废话貌似说的有点多。。。。 Requests库安装及七个基本方法​ python提供了安装包管理工具pip，有点类似于Sublime text的Package Control，pip安装的时候注意版本的兼容，建议选择源码编译安装，解压的个人觉得容易出问题，具体请自行百度。有了pip以后就简单了，打开cmd直接输入键入命令pip install requests(此处针对的是windows环境，mac什么的应该更加方便就不说了)。然后检查一下是否已经成功安装。 1234import requestsr = requests.get("http://www.baidu.com")print(r.status_code)print(r.text) 这里我用的是Sublime Text，其实用python自带的idle也挺方便的，但我个人不习惯呃。 如果打印出状态码为200而且输出的百度的文本内容，说明已经requests库已经到位，否则请回去检查一下安装是否出了问题。 其中requests.request()是基础，构造请求。然后其他6个分别是： get：获取HTML网页的主要方法 head: 获取HTML头部信息。 这是最为重要的两个，还有post, put, patch, delete。这6个方法均对应着HTTP协议的操作(可以说requests库就是为HTTP而生的)。 requests.request()​ 标准形式是：requests.request(method, url, **kwargs)，method对应上面的post, put等方法，而**keargs是控制参数。比如这样写： 123kv = &#123;'key1': 'value1','key2': 'value2'&#125;r = requests.request('GET', 'url', params = kv)print(r.url) 会打印出url?key1=value1&amp;key2=value2。最重要的是伪造header，这样写： 12hd = &#123;'user-agent':'Mozilla/5.0'&#125;r = requests.request('GET', 'url', heaers = hd) 其他还有json，data, cookies, auth, timeout等。 requests.get()​ 最常见的写法：r = requests.get(url),r表示的是服务器返回的一个Response对象，url即目标网页，完整的写法是这样的：r = requests.get(url, params = None, **kwargs)，后面两个为选填参数，其中params表示字典或者字节流格式，**keargs表示12个控制的参数。 ​ 有意思的是，response(下面简称r)对象不仅包括服务器返回的信息，也包括requests请求的信息。r的属性主要有这么5个： r.status_code: 200表示成功，404表示失败，更多状态码自己查查看。 r.text: HTTP响应内容的字符串形式。 r.encoding: 编码方式 r.apparent_encoding: 备选编码方式 r.content: HTTP响应内容的二进制形式 其中encoding是根据header里面的charset字段来判断的，如果没有，会默认为ISO-8859-1，此编码下中文会乱码，备选编码方式一般是utf-8。 Requests库的异常​ 嵩老师说了一句：网络连接有风险，异常处理很重要。错误有很多种，DNS查询失败，HTTP 错误，URL错误，超过最大重定向次数导致重定向异常，超时错误等等。只要状态不是200就可以认为是HTTP错误了。 通用代码框架1234567891011121314import requestsdef getHTMLText(url): try: r = requests.get(url) r.raise_for_status()#不是200就会引发异常 r.encoding = r.apparent_encoding return r.text except: return "异常"if __name__ == "__main__": url = "" print(getHTMLText(url)) ​ 不得不说，这代码比较严谨，不像我前面两次的那样，哈哈哈，从开始就养成好习惯。 盗亦有道——robots协议爬虫尺度分类与可能带来的问题​ 网络爬虫是有尺度的，大概可以分为这么三种： 规模小，数据量小，速度不敏感，用requests库即能完成，用来玩转网页的。 中等规模，爬取速度敏感，用scrapy库，爬取系列网站。 大规模，用搜索引擎，定制开发。爬取全网 网络爬虫会带来的三个问题： 性能骚扰：水平太烂的话会给服务器资源造成巨大的压力 法律风险：如果爬来的数据用于商业用途可能要面对法律风险 隐私泄露；爬到正常情况下无权限获取的数据可能会造成个人隐私泄露 网络爬虫的限制来源审查：判断User-Agent，只允许友好爬虫和浏览器访问 发布公告：robots协议，告知爬虫，要求遵守。 robots协议全名叫：Robots Exclusion Standard，网络爬虫排除标准。以协议约定的形式告知爬虫爬取权限，一般在网站的根目录下。来看一下京东的robots协议： 12345678910111213&gt;User-agent: * &gt;Disallow: /?* &gt;Disallow: /pop/*.html &gt;Disallow: /pinpai/*.html?* &gt;User-agent: EtaoSpider &gt;Disallow: / &gt;User-agent: HuihuiSpider &gt;Disallow: / &gt;User-agent: GwdangSpider &gt;Disallow: / &gt;User-agent: WochachaSpider &gt;Disallow: /&gt; *代表所有，/代表根目录 原则上是应该要遵守robots协议的，但如果访问量很小，类似人的上网行为，可以不考虑遵守robots协议。但是！！！如果是商业用途或者爬取全网，必须遵守robots协议，不然出了问题是真的要背法律责任的。 举个栗子保存一张图片： 12345678910111213141516171819import requestsimport osurl = "https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1832529435,1519198910&amp;fm=11&amp;gp=0.jpg"root = "D://pics//"path = root + url.split('/')[-1]try: if not os.path.exists(root): os.mkdir(root) if not os.path.exists(path): r = requests.get(url) with open(path, 'wb') as f: f.write(r.content) f.close() print("文件保存成功") else: print("文件已存在")except: print("爬取失败") root是目录，先检测目录是否存在，如果没有，建一个，然后路径同理，接着打开一个个文件，然后写入r.content(因为content属性是二进制形式的) 嗯？保存一张图片也许没有右键另存为来的快，但成百上千张图片可就无能为力了，特别是还要对图片进行筛选的情况下。(我存的图片男生可以参考，女生可以换一张，哈哈哈哈哈哈哈) Beautiful Soup库入门​ 懒得再新建文章了，直接继续第二章的内容。这次是学习Beautiful Soup库，(形象地称之为煲汤。。。。是的，没错，老师也用了这个词。。。。)自行安装 测试是否安装成功： 1234567import requestsfrom bs4 import BeautifulSoupr = requests.get("http://python123.io/ws/demo.html")demo = r.textsoup = BeautifulSoup(demo, 'html.parser')print(soup.prettify()) 如果打印出来的内容和HTML文档排版一样就OK。 ##Beautiful Soup库的基本元素 ​ 首先，Beautiful Soup库是解析、遍历、维护“标签树”的功能库，这里会对应HTML的框架(Requests库对应HTTP操作)。 ​ Beautiful Soup库的引用一般是BeautifulSoup类，有意思的是，真正用的时候只要写bs4就好，from bs4 import BeautifulSoup,就是这样奇葩。而上面代码中soup = BeautifulSoup(demo, &#39;html.parser&#39;),html.parser则指的是html解析器。用来解析demo的HTML结构。 ​ Beautiful Soup库有如下5个基本元素： Tag: 最基本的信息单元 Name: 标签的名字，格式：.name Attributes: 标签的属性。格式：.attrs NavigableString: 标签内非属性字符串。格式：.string Comment: 标签内部的注释信息，一种特殊的comment类型 Tag 标签​ 任何HTML语法中的标签都可以通过soup.tag访问，比如，soup.title，soup.a等等当存在多个相同的tag时，返回第一个 ​ 每个标签都有自己的名字，可以这样来访问：soup.a.name，还可以查看其上一级标签名字soup.a.parent.name，父亲的父亲同理，儿子的话用children就行. ​ 标签属性是字典形式，可以这样来访问：tag = soup.a,tag.attrs，打印出一个字典后，还可以查看具体的某一个属性，比如tag.attrs[&#39;class&#39;]。 ​ NavigableString 和 Comment不再细说，原理和这个一样。 基于bs4库的HTML内容遍历方法​ 贴一下HTML的基本结构 123456789101112131415161718192021222324&lt;html&gt; &lt;head&gt; &lt;title&gt; This is a python demo page &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="title"&gt; &lt;b&gt; The demo python introduces several python courses. &lt;/b&gt; &lt;/p&gt; &lt;p class="course"&gt; Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses: &lt;a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1"&gt; Basic Python &lt;/a&gt; and &lt;a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2"&gt; Advanced Python &lt;/a&gt; &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 标签树的三个属性可以做到下行遍历： .contents：将tag的所有子节点都存入列表 .children：子节点的迭代类型，用循环遍历子节点 .descendants：子孙节点的迭代烈性，包括所有子孙节点，用循环遍历 遍历子节点和子孙节点： 12345for child in soup.body.children: print(child) for child in soup.body.descendants: print(child) ​ 上行遍历： .parent：节点的父节点 .parents：节点父辈的迭代类型，循环遍历 上行遍历示例： 12345for parent in soup.a.parents: if parent is None: print(parent) else: print(parent.name) 考虑到会遍历到根节点(即soup本身)，所以做了一个判断。 平行遍历： .next_sibling：按照顺序遍历下一个平行节点 .previous_sibling：按照顺序遍历上一个平行节点 .next_siblings：迭代，循环遍历后面所有平行节点 .previous_siblings：迭代，循环遍历前面所有平行节点 注意一个问题：在同一个父节点下才算是平行节点。 比如，这是遍历后续节点： 12for sibling in soup.a.next_sibling: print(sibling) 友好输出：Prettify()​ 很简单，直接print(soup.prettify())，而且支持utf-8编码. 信息标记与提取信息标记上表就好： XML：最早的通用信息标记语言，可扩展性强但是繁琐 JSON：信息有类型，键值对显示，超级适合js的处理，比XML简洁 YAML：信息无类型，文本信息比例高，可读性很好。 作用： XML：Internet上的信息交互与传递 JSON：移动应用端与节点的信息通信 YAML：系统的配置文件 信息提取​ 最好的方法当然是形式解析与关键字内容搜索融合。 ​ 比如，要提取所有的URL链接，只需要这样： 12for link in soup.find_all('a'): print(link.get('href')) ​ find_all(name, attrs, recursive, string, **kwargs)，参数分别是标签名字，标签属性，是否对子孙全部检索(默认为True)，检索区域，及控制参数，既然是匹配，肯定是正则大法好咯。TIPS：(…) 等价于 .find_all()，soup(…) 等价于 soup.find_all() 再举个栗子​ 爬取中国大学排名，很简单的栗子，直接上代码好了： 1234567891011121314151617181920212223242526272829303132333435import requestsfrom bs4 import BeautifulSoupimport bs4def getHTMLText(url): try: r = requests.get(url, timeout = 30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return ""def fillUnivList(ulist, html): soup = BeautifulSoup(html, "html.parser") for tr in soup.find('tbody').children: if isinstance(tr, bs4.element.Tag): tds = tr('td') ulist.append([tds[0].string, tds[1].string, tds[3].string])def printUnivList(ulist, num): tplt = "&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;" print(tplt.format("排名", "学校名称", "总分",chr(12288))) for i in range(num): u = ulist[i] print(tplt.format(u[0], u[1], u[2], chr(12288)))def main(): unifo = [] url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html' html = getHTMLText(url) fillUnivList(unifo, html) printUnivList(unifo, 30)#30所大学main() ​ tplt和chr(12288)是为了使输出稍微美观一点而做的一点小优化。中间要判断是否是bs4.element.tag类型，所以引用了bs4库。(吐槽一下python的format是真的强，还有，南航居然排到了29诶。。。) Re(正则表达式)入门​ 前面已经有过接触，正则表达式是一种针对字符串表达“简洁”和“特征”思想的工具，用来判断某字符串的的特征归属，可以用来替换或者匹配字符串。我决定直接贴图… ​ 调用方式：import re，re库一般匹配raw string(原生字符串类型r’text’)，即不包括对转义符再次转义的字符串。re库也可以直接用string类型来表示，但会更繁琐，如果正则包含转义符时，用raw string类型更好。 ​ 主要函数： ​ 大部分函数都是三个参数：pattern, string, flags。pattern即为构造的正则表达式，string为待匹配字符串，flags为控制参数，其中split函数还有一个maxsplit(最大切割次数)，放在string和flags中间。sub函数更特殊，re.sub(pattern, repl, string, count = 0, flags = 0), 另外常用的正则可以经过编译使用：pat = re.compile(pattern, flags), rst = pat.search(&#39;string&#39;),可以提升运行速度。返回的是match对象，它的属性如下： ​ ​ 属性的方法： ​ 另外，在默认返回一个对象而又能同时匹配多个字符串时，默认返回最长的那个(贪婪匹配)，如果有 需要，作最短匹配，加一个?就可以了。 ​ 定向爬取淘宝商品(不要不加限制地爬，robots协议不允许….) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import requestsimport redef getHTMLText(url): try: r = requests.get(url, timeout = 30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return ""def parsePage(ilt, html): try: plt = re.findall(r'\"view_price\"\:\"[\d\.]*\"', html) tlt = re.findall(r'\"raw_title\"\:\".*?\"', html) for i in range(len(plt)): price = eval(plt[i].split(':')[1]) title = eval(tlt[i].split(':')[1]) ilt.append([price, title]) except: print("")def printGoodsList(ilt): tplt = "&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;" print(tplt.format("序号", "价格", "商品名称")) count = 0 for g in ilt: count = count + 1 print(tplt.format(count, g[0], g[1]))def main(): goods = '书包' depth = 3 start_url = 'https://s.taobao.com/search?q=' + goods infoList = [] for i in range(depth): try: url = start_url + '&amp;s=' + str(44 * i) html = getHTMLText(url) parsePage(infoList, html) except: continue printGoodsList(infoList)main() ​ 上一段比较优美的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import requestsfrom bs4 import BeautifulSoupimport tracebackimport re def getHTMLText(url, code="utf-8"): try: r = requests.get(url) r.raise_for_status() r.encoding = code return r.text except: return "" def getStockList(lst, stockURL): html = getHTMLText(stockURL, "GB2312") soup = BeautifulSoup(html, 'html.parser') a = soup.find_all('a') for i in a: try: href = i.attrs['href'] lst.append(re.findall(r"[s][hz]\d&#123;6&#125;", href)[0]) except: continue def getStockInfo(lst, stockURL, fpath): count = 0 for stock in lst: url = stockURL + stock + ".html" html = getHTMLText(url) try: if html=="": continue infoDict = &#123;&#125; soup = BeautifulSoup(html, 'html.parser') stockInfo = soup.find('div',attrs=&#123;'class':'stock-bets'&#125;) name = stockInfo.find_all(attrs=&#123;'class':'bets-name'&#125;)[0] infoDict.update(&#123;'股票名称': name.text.split()[0]&#125;) keyList = stockInfo.find_all('dt') valueList = stockInfo.find_all('dd') for i in range(len(keyList)): key = keyList[i].text val = valueList[i].text infoDict[key] = val with open(fpath, 'a', encoding='utf-8') as f: f.write( str(infoDict) + '\n' ) count = count + 1 print("\r当前进度: &#123;:.2f&#125;%".format(count*100/len(lst)),end="") except: count = count + 1 print("\r当前进度: &#123;:.2f&#125;%".format(count*100/len(lst)),end="") continue def main(): stock_list_url = 'http://quote.eastmoney.com/stocklist.html' stock_info_url = 'https://gupiao.baidu.com/stock/' output_file = 'D:/BaiduStockInfo.txt' slist=[] getStockList(slist, stock_list_url) getStockInfo(slist, stock_info_url, output_file) main() ​ 动态优化好评。]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取网易云热评]]></title>
    <url>%2F2017%2F09%2F14%2F%E7%88%AC%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E7%83%AD%E8%AF%84%2F</url>
    <content type="text"><![CDATA[爬取网易云热评，用python上手真心简单，怪不得有人总说“人生苦短，我用python“。 ​ 爬网页数据的话一般是解析html的结构，这种适合抓取html里面多种元素的情况，而我只是想看个热评而已，可以另辟蹊径：直接搞到评论的API，然后获取json返回，最后解析就行了。((￣▽￣)~所以说很简单咦) 找到API​ 打开网易云网页版，输入自己想抓取热评的歌曲，然后用开发者工具，输入comments就能找到评论API的url了，点response就能看到json格式的评论了。 那个数字代表的应该就是这首歌的id了，后面有一个叫csrf_tocken的参数，显示为空，那就不管它了。请求方式依然是post，然后表单那里还有两个加密过的参数，分别是params和encSecKey，一开始以为是每首歌都会对应这么一个参数，后来发现并不是，刷新了页面后却发现这个参数变了，可能是对应评论页码加密的？ 获取json​ 再一次慨叹一下python库的强大，不信？那你看代码长度咯。 1234567import requestsimport jsonurl = 'http://music.163.com/weapi/v1/resource/comments/R_SO_4_63650?csrf_token='param = &#123;'params':'', 'encSecKey':''&#125;r = requests.post(url, param)data = r.text #拿到json 这样就拿到评论的json格式了(PS：那两个参数实在是太长了，贴出来看的我不舒服，写的时候自行粘贴上去就好)。 解析json拿到了json后自然要解析一波，python的json库可以很轻易地帮我们解决这个问题。首先从开发者工具那直接复制json，拿到这里在线json校验格式化工具,看一下json的具体形式(直接那么长一串简直反人类没法看)。 舒服多了23333。主要信息有nickname，content，likedCount把这三个拎出来就差不多了。(id, 内容, 点赞量)json库不会用的话去查一下就好，上手很容易的，然后直接解析后打印就好了。(就一页热评，懒得写入文件了。)稍微完整的代码： 1234567891011121314151617import requestsimport jsonurl = 'http://music.163.com/weapi/v1/resource/comments/R_SO_4_63650?csrf_token='param = &#123;'params':'', 'encSecKey':''&#125;r = requests.post(url, param)data = r.text #拿到jsonjsOb = json.loads(data)hotComments = jsOb['hotComments']for i in range(len(hotComments)): user_nickname = hotComments[i]['user']['nickname'] likedCount = hotComments[i]['likedCount'] content = hotComments[i]['content'] print('评论',str(i+1),' 用户名:',user_nickname,'喜 欢：',str(likedCount)) print('-----------------------------') print(content) 这样就OK咯。贴一下我抓的《独家记忆》第一页热评。 emmmmm，后期再更新一下抓全部评论和整个歌单的热评好了。不过那样数据会比较大，时间相应也挺长的，不太敢多开线程加快速度，怕触发反爬虫机制ip被封了就GG了。]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫初体验]]></title>
    <url>%2F2017%2F09%2F11%2Fpython%E7%88%AC%E8%99%AB%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[python基础部分看了一遍后，感觉需要找点乐子，于是想到去学一点爬虫批量抓图片，嘿嘿嘿 先介绍一下python的两大利器(库)：requests和re(emmmm，可别以为后面那个是前者的缩写，其实半毛钱关系都没有)。requests库有多厉害呢？简单地说就是python 的http库，可以帮开发人员省很多代码，具体有多厉害呢？戳这里 。而re库则是regexp的缩写，python提供了相当强大的正则表达式引擎，很大程度上简化了python代码，稍微详细一点的介绍在这里 。好了以下是正文 获取网页12345import requestsheader = &#123;'User-Agent':'xxx'&#125;url='https://www.xxxxx'r = requests.get(url,headers = header)txt = r.text 先引入 requests模块，然后header是用来伪造浏览器UA的，用Chrom的话直接去开发者工具那找一下自己的Request Headers就好，url则是要抓取的网页地址，传入这两个参数给.get函数后，就能拿到初步结果 解析图片嗯，别忘了自己是来抠图的，上面那个函数写完后，一个print(text)出来的却是一大坨HTML，这当然不是我要的，去网上找了一下发现Python还有个叫Beautiful Soup的东西(→_→我第一反应是煲汤)，这个库是用来解析HTML结构的，看了几眼觉得有点复杂，这时候想起了正则大法，图片链接不都是有明显特征的嘛，于是考虑正则。匹配以https://开头以.jpg类结尾的，并作最短匹配，然后全部累塞进数组。 12345678910jpg = re.compile(r'https://[^\s]*?\.jpg')jpeg = re.compile(r'https://[^\s]*?\.jpeg')gif = re.compile(r'https://[^\s]*?\.gif')png = re.compile(r'https://[^\s]*?\.png')imgs=[]imgs+=jpg.findall(txt)imgs+=jpeg.findall(txt)imgs+=gif.findall(txt)imgs+=png.findall(txt) 下载图片request用来存储get的url，判断状态码是否正常，如果正常就写入文件。 12345678910def download(url): request = requests.get(url) if req.status_code == requests.codes.ok: name = url.split('/')[-1] f = open("./"+name,'wb') f.write(req.content) f.close() return True else: return False 写好下载函数后就可以写循环来挨个下载了 12345678errors = []for img_url in imgs: if download(img_url): print("download :"+img_url) else: errors.append(img_url)print("error urls:")print(errors) 还可以设置一个urls数组，同时下载多个网页的图，download函数也可以加个文件夹名，这样就可以了。 最后我干了什么呢？上个图，自己体会 以上代码需要优化，直接用的话还有些不完善，需要根据实际的网站来调整。 多说一句​ 想起一位厉害小姐姐的训示：少年的你的梦想应该是拯救世界，怎么能每天爬一些日韩女人啊爆照贴妹子什么的。 ​ (逃：]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10改掉C盘下的中文用户名]]></title>
    <url>%2F2017%2F09%2F11%2Fwin10%E6%94%B9%E6%8E%89C%E7%9B%98%E4%B8%8B%E7%9A%84%E4%B8%AD%E6%96%87%E7%94%A8%E6%88%B7%E5%90%8D%2F</url>
    <content type="text"><![CDATA[昨天早上一起来，正愉快地折腾着，突然发现有些报错无论如何都解决不了，有的虽然影响不大但看起来很烦，搜索一波后发现其实是我c盘下中文路径搞的鬼， 嗯，话不多说就是干。 搜了一会儿后发现这个世界上的误解还真多，居然有这么多人以为是改本地的管理员账号，毫不客气地给了一个”踩”。终于在知乎下面找到了一点眉目。（还是比百度强） 懒得传图，直接复制一下第一个答案： 假设原用户名为 小明，需改为 xiaoming。 1.先新建一个管理员账户，然后注销当前用户，以新建的管理员账户登录； 2.重命名 c:\Users\小明 为 c:\Users\xiaoming； 3.打开注册表编辑器（win+R 输入 regedit），定位到 HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ProfileList 的某一子项（S-1-5-21… 开头的），将“数据名称”为 ProfileImagePath 的“数值数据”内容 C:\Users\小明 改为 C:\Users\xiaoming ； 4.改后以原账户登录并删除新建账户。 咦，下面有80+评论，去瞄了一眼，虽然成功的人也有，但失败的也不少，我还是没有直接按照这个方法改，但大概思路已经清楚了，即临时存一下当前中文名下的东西，拿到改名权限后改掉中文名，再换回来。 在下面的回答中找到了一位小姐姐的博客园链接，给链接的人称此方法很成功，嗯，大概说一下这个方法的步骤： 改注册表用win+r打开命令窗口，输入regedit，把系统注册表调出来，然后去这个地方 HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ProfileList 依次点开那些以S-1-5-开头的项，找到ProfileImagePath，把C：\Users\中文名 改成C：\Users\English name 记得把所有的Path都改掉，确认后关掉注册表。 改文件名打开cmd后发现依旧是中文名，这个当然，因为只是改完了注册表，文件权限还没转移，于是重启电脑。 启动后发现壁纸什么的都不一样了，这是因为当前Users 已经变成了TEMP，即临时系统账户。这时候系统会提醒你无法连接到你的账户，嗯，因为你的注册表已经改了，点击隐藏， 别点注销，然后按ctrl + e 打开我的电脑，找到C：\Users\中文名，修改文件名，点击给予权限。然后重启电脑就大功告成啦 可能有的问题更改之前就已经安装的软件可能依旧会显示中文路径，如果不报错的话就没关系，（强迫症选手请重装软件）。其他的问题好像没了，那个小姐姐还给了测试。 这是小姐姐的文章链接]]></content>
      <categories>
        <category>环境</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大和子序列]]></title>
    <url>%2F2017%2F09%2F10%2F%E6%9C%80%E5%A4%A7%E5%92%8C%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[今天来看一个简单的问题，求最大的和子序列/求最大和子数组 题目是这样的：已知序列：-2, 11, -4, 13, -5, 2, -5, -3, 12, -9，求此序列的最大子序列和 ​ 其实题目很简单，但智障的我一开始弄错了，直接把所有负数提出去然后把剩下的相加，这也太简单了点吧。。。。后来想想，貌似不太对，于是，重来。一共用了三种方法。(名字都是我瞎写的) 方法一：暴力法​ 没错，就是直接把这个数组的所有子序列的和都算一遍，跟初始最大值比较，代码如下： 12345678910111213141516171819int main()&#123; int a[] = &#123;-2,11,-4,13,-5,2,-5,-3,12,-9&#125;; int maxSum = a[0],n = sizeof(a)/sizeof(a[0]); for(int i = 0;i &lt; n;i++)//子数组长度 &#123; for(int j = 0;j &lt; n;j++)//子数组开始的位置，数组下标 &#123; int sum = 0;//记录当前子数组和 for(int k = j;k &lt; n&amp;&amp;k &lt; j + i;k++)//求和 &#123; sum += a[k]; &#125; if(sum &gt; maxSum) maxSum = sum; &#125; &#125; cout &lt;&lt; "子序列的最大和是："&lt;&lt; maxSum &lt;&lt; endl; return 0;&#125; 对，就是这么暴力，效率很低，时间复杂度：O(n³) 方法二：递进求和​ 不断求出以a[i]开头的子序列的和，并在求的过程中记录好最大的子序列的和，函数代码如下： 1234567891011121314int maxSubArraySum(int *arr,int n)&#123; int i,j,maxSum = 0,sum; for(i = 0;i &lt; n;i++)//子数组开始位置 &#123; sum = 0; for(j = i;j &lt; n;j++) &#123; sum += arr[j]; if(sum &gt; maxSum) maxSum = sum;//求和并比较 &#125; &#125; return maxSum; &#125; 相对方法一来说，方法二减少了一次遍历，时间复杂度为：O(n²) 方法三：判断求和​ 仔细想一下，一个数，加上一个负数会变小，加上零不变，加上正数才会变大，对，就是这么简单的道理，就可以用来优化这个题的算法了。从a[0]开始累加，如果大于初始值，就替换，如果和小于零，直接舍弃，然后是a[1]，函数代码如下： 1234567891011int maxSubArraySum_2(int *arr,int n)&#123; int i,maxSum = arr[0],sum = 0; for(i = 0;i &lt; n;i++)//子数组开始位置 &#123; sum += arr[i]; if(sum &gt; maxSum) maxSum = sum;//记录最大累加和 if(sum &lt; 0) sum = 0;//累加和小于零的不要 &#125; return maxSum;&#125; 这样的话，只要对数组遍历一次就能解决了，时间复杂度降为O(n)，最简单道理往往有意想不到的效果，哈哈哈哈。 另外，加个tips: n = sizeof(a)/sizeof(a[0])，Strlen()函数不适用于整数数组]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于排序]]></title>
    <url>%2F2017%2F09%2F10%2F%E5%85%B3%E4%BA%8E%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[我目前已经学会的排序以及理解（会不断更新） 先贴个交换函数： 123456void Swap(int A[],int i,int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125; 冒泡排序这是进大学后c语言课本上介绍的第一个排序，也是最简单，最容易理解的一个排序，顾名思义，就像冒泡一样，一次一次把最值往最后面放，完成排序。函数代码如下： 12345678910111213void BubbleSort(int A[],int n)&#123; for(int j = 0;j &lt; n - 1;j++) &#123; for(int i = 0;i &lt; n - 1 - j;i++) &#123; if(A[i] &gt; A[i + 1]) //从小到大排序 &#123; Swap(A,i,i + 1); &#125; &#125; &#125;&#125; 冒泡排序是稳定的，因为即使有相同的数也不会打乱原来是次序，平均时间复杂度：O(n²)。 选择排序​ 相比于相邻交换的冒牌排序，选择排序是通过从未排序的数据元素中选出最值放在该序列的起始位置，直到所有元素排完，同样需要循环两次，无法优化时间。代码如下： 123456789101112131415161718void SelectionSort(int A[],int n)&#123; for(int i = 0;i &lt; n - 1;i++) &#123; int min = i; for(int j = i + 1;j &lt; n;j++) &#123; if(A[j] &lt; A[min]) &#123; min = j; &#125; &#125; if(min != i) &#123; Swap(A,min,i); &#125; &#125;&#125; 选择排序是不稳定的，因为如果有相同的数的话是可以改变原来次序的，平均时间复杂度：O(n²)。 插入排序​ 看到这个方法，我的第一反应便是抓扑克牌，原理和抓牌原理一样，即，左手上的牌是已经排好序了的，将左手上的牌依次和抓到的牌比较，如果大于抓到的牌便把这张牌左移，然后插入抓到的牌。函数代码如下: 1234567891011121314void InsertionSort(int A[],int n)&#123; for(int i = 1;i &lt; n;i++) &#123; int get = A[i]; int j = i - 1; while(j &gt;= 0 &amp;&amp; A[j] &gt; get) &#123; A[j + 1] = A[j]; j--; &#125; A[j + 1] = get; &#125;&#125; 相同的牌不影响顺序，插入排序是稳定的，平均时间复杂度：O(n²)。 快速排序​ 快速排序基于一种二分的思想，即以一个数为基准数，不断将数组二分，最终当所有基准数都归位后，排序也就完成了。快速排序之所以较快，是因为每次交换都是跳跃式的。函数代码如下：(千万注意下标是从0开始的 ) 12345678910111213141516171819202122void quicksort(int arr[],int left,int right)&#123; if(left &gt; right) return; int i,j,temp; temp = arr[left];//temp为基准数 i = left; j = right; while(i != j) &#123; //基准数在左边，所以要从右边开始找 while(arr[j] &gt;= temp &amp;&amp; i &lt; j)j--; //再从左往右找 while(arr[i] &lt;= temp &amp;&amp; i &lt; j)i++; //如果没有相遇。就交换 if(i &lt; j) Swap(arr,i,j); &#125; arr[left] = arr[i]; arr[i] = temp; quicksort(arr,left,i - 1);//继续处理左边 quicksort(arr,i + 1,right);//继续处理右边 &#125; 这两个等我会了再写233333 归并排序归并排序的原理是分治法，简单点说就是把一个序列拆成多个子序列，将子序列排好序后，再将其合并为一个序列。归并排序的效率也比较可观，达到了o(NlogN)。 1234567891011121314void mergeArray(int a[], int first, int mid, int last, int temp[])&#123; int i = first, j = mid + 1; int m = mid, n = last, k = 0; while(i &lt;= m &amp;&amp; j &lt;= n) &#123; if(a[i] &lt;= a[j]) temp[k++] = a[i++]; else temp[k++] = a[j++]; &#125; while(i &lt;= m) temp[k++] = a[i++]; while(j &lt;= n) temp[k++] = a[j++]; for(i = 0;i &lt; k;i++) a[first + i] = temp[i];&#125; 12345678910void mergeSort(int a[], int first, int last, int temp[])&#123; if(first &lt; last) &#123; int mid = (first + last) / 2; mergeSort(a, first, mid, temp);//处理左边 mergeSort(a, mid + 1,last, temp);//处理右边 mergeArray(a, first, mid, last, temp);//合并 &#125;&#125; 堆排序]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Sublime配置成python环境]]></title>
    <url>%2F2017%2F07%2F19%2F%E5%B0%86Sublime%E9%85%8D%E7%BD%AE%E6%88%90pythona%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[刚开始学python的时候，用的是python自带的IDLE.bat，总感觉太简陋，字体很小，也没有自动补全的东西，之前有段时间在写网页玩，一直用着Sublime text 3——这款轻量而且扩展性很强而且颜值高的编辑器，想着在St3上写python岂不是美滋滋了，于是搜索了一波，最后发现了St3直接提供了一个强大的插件——Anaconda，可以将St3打造成一个python IDE。 下载好python和Sublime text 3打开安装包管理工具（这个还不会的小白请自行百度，不太好截图），输入Install Package，然后输入Anaconda，回车安装保存文件名为.py后缀后就会发现写python会有高亮和自动补全了。可能遇到的问题 用ST3一打开python文件就有这个问题，发现后面写着try to set the ‘swallow_startup_errors’ to ‘true’ 然后发现这个json的设置居然是在SublimeREPL里面而不是Anaconda。。。 好吧不管了 先试试再说 进入Preferences–package settings–&gt;SublimeRPEL–&gt;settings user 键入： “swallow_startup_errors”: true, 保存后重启。然鹅并没有什么卵用…能解决这个问题的大佬可以来教一下 还有可能遇到这种情况： 大白框和小白点，猜想可能是因为python的缩进？ （你自己喜欢的话就没关系，不用改了） 我个人看着很不舒服，搜索后发现这个是默认开启的功能，点开Preferences-Pacakage Settings-Anaconda-Settings user 加上如下一句： 即可解决这个问题. 然后就可以愉快地用St3写Python了。（ctrl + B 即可运行） 嗯，故事就这么结束了么？当然没有 然后就遇到了第一个坑：Input无效 Sublime似乎无法完成input这种交互式命令 解决方法请参考此链接]]></content>
      <categories>
        <category>环境</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Sublime</tag>
      </tags>
  </entry>
</search>
